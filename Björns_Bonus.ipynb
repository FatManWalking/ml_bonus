{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Björns Bonus.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatManWalking/ml_bonus/blob/main/Bj%C3%B6rns_Bonus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI316RGyTNUi"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox --quiet\n",
        "\n",
        "#!pip install pytorch==1.8.1 --quiet"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIbDP4HcVf0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6272fa-c8ca-44eb-befd-36b2e70ee294"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdBwR0DdlvMC"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRfw8A8lTfIA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Sequential\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMtaDVeSqtiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51960865-d822-4ea0-d02a-d34c82c6cd65"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2cgGYdu3vPI"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzfSxX_z05Z7"
      },
      "source": [
        "# Get victim\n",
        "We only know that this classifier classifies bees and ants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHqKj1Ho04sc"
      },
      "source": [
        "victim = models.resnet18(pretrained=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd69tBAJ13Kd"
      },
      "source": [
        "victim = torch.load('/content/drive/MyDrive/DHBW Vorlesungen/6. Semester/Daten für Projekte/modelFinetune.pth')\n",
        "victim = victim.to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pg6qmvm215V"
      },
      "source": [
        "# Get the batch loader and set up our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPImhjTU1Wb"
      },
      "source": [
        "def get_data_loader(batch_size=20):\n",
        "    \"\"\"\n",
        "    define data augmentation and return data loader with the cifar10 dataset loaded\n",
        "    parameters:\n",
        "        batch_size: size of the batch used for training and validation \n",
        "                    size of test batch is always 10\n",
        "    Return:\n",
        "        3 pytorch.dataloaders for easy batching of the dataset (train, validation and test)\n",
        "    \"\"\"\n",
        "\n",
        "    # data augmentation\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize([224,224]), # Resizing the image as the VGG only take 224 x 244 as input size\n",
        "            transforms.RandomHorizontalFlip(), # Flip the data horizontally\n",
        "            #TODO if it is needed, add the random crop\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    \"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "        \n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "    test = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=False, transform=transform_test)\n",
        "    \n",
        "    n = len(trainset)\n",
        "    print(n)\n",
        "    train, val =torch.utils.data.random_split(trainset, \n",
        "                                                    [int(n//100*60), int(n//100*40)],\n",
        "                                                    generator=torch.Generator().manual_seed(42))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=20, shuffle=True, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivfuzCoOU6JH",
        "outputId": "6cae5ad0-22cc-4150-cb89-c3e03f932e95"
      },
      "source": [
        "train_loader, val_loader, test_loader = get_data_loader(batch_size=20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8aEy8hiU8oC"
      },
      "source": [
        "model_name = 'vgg11_bn'\n",
        "num_classes = 2"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QznaTxNvVKde"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name='vgg11', num_classes='10', feature_extract=True, use_pretrained=True):\n",
        "        \"\"\"\n",
        "        initalizes the version of VGG to be used and freezes the layers of the model and then add a new unfrozen head\n",
        "\n",
        "        params:\n",
        "            model_name(str): for example vgg16_bn for VGG-net with 16 weight layers and batch normalization\n",
        "            num_classes(int): number of output classes (100 when using CIFAR100)\n",
        "            feature_extract(bool): should the layers be frozen or not\n",
        "            use_pretrained(bool): use the pretrained model weights or only the architecture itself\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if model_name == 'vgg11':\n",
        "            model = models.vgg11(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg11_bn':\n",
        "            model = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg16':\n",
        "            model = models.vgg16(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg16_bn':\n",
        "            model = models.vgg16_bn(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg19':\n",
        "            model = models.vgg19(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg19_bn':\n",
        "            model = models.vgg19_bn(pretrained=use_pretrained)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model\n",
        "        \n",
        "        \n",
        "        self.set_parameter_requires_grad(feature_extract)\n",
        "        num_ftrs = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.feature_extract = feature_extract\n",
        "        self.input_size = input_size\n",
        "    \n",
        "    def get_params_to_update(self):\n",
        "        \"\"\" \n",
        "        prints the list of weights and bias that not frozen and will be tuned\n",
        "        \n",
        "        Gather the parameters to be optimized/updated in this run. If we are\n",
        "        finetuning we will be updating all parameters. However, if we are\n",
        "        doing feature extract method, we will only update the parameters\n",
        "        that we have just initialized, i.e. the parameters with requires_grad\n",
        "        is True.\n",
        "        \"\"\"\n",
        "\n",
        "        params_to_update = self.model.parameters()\n",
        "        print(\"\\tWeights and Bias to be tuned:\")\n",
        "        if self.feature_extract:\n",
        "            params_to_update = []\n",
        "            for name,param in self.model.named_parameters():\n",
        "                if param.requires_grad == True:\n",
        "                    params_to_update.append(param)\n",
        "                    print(\"\\t\\t\",name)\n",
        "        else:\n",
        "            for name,param in self.model.named_parameters():\n",
        "                if param.requires_grad == True:\n",
        "                    print(\"\\t\\t\",name)\n",
        "                    \n",
        "        return params_to_update\n",
        "    \n",
        "    def set_parameter_requires_grad(self, feature_extracting):\n",
        "        \"\"\"\n",
        "        this freezes the layers in the network\n",
        "        the new layer added later will automatically be unfrozen\n",
        "        \"\"\"\n",
        "        if feature_extracting:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "\n",
        "    # The trick is instead of using the real label we let the victim classify it first and use that label to train the classifier\n",
        "    def train_model(self, train_loader, val_loader, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "        since = time.time()\n",
        "\n",
        "        val_acc_history = []\n",
        "        val_loss_history = []\n",
        "\n",
        "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "        best_acc = 0.0\n",
        "        stop = False\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    self.model.train()  # Set model to training mode\n",
        "                    dataloader = train_loader\n",
        "                else:\n",
        "                    self.model.eval()   # Set model to evaluate mode\n",
        "                    dataloader = val_loader\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, _ in dataloader:\n",
        "                    inputs = inputs.to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        labels = np.argmax(victim(inputs).to('cpu'), axis=1)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Get model outputs and calculate loss\n",
        "                        \n",
        "                        outputs = self.model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                epoch_loss = running_loss / len(dataloader.dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "                if phase == 'val':\n",
        "                    val_acc_history.append(epoch_acc)\n",
        "                    # A simple addition to stop the training early\n",
        "                    val_loss_history.append(loss)\n",
        "                    if len(val_loss_history) > 3:\n",
        "                        stop = (abs(val_loss_history[-3] - val_loss_history[-3]) +\n",
        "                                abs(val_loss_history[-1] - val_loss_history[-2])) <= 0.001\n",
        "                        print(abs(val_loss_history[-3] - val_loss_history[-3]) +\n",
        "                                abs(val_loss_history[-1] - val_loss_history[-2]))\n",
        "                if stop:\n",
        "                    break\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "        # load best model weights\n",
        "        self.model.load_state_dict(best_model_wts)\n",
        "        torch.save(self.model, 'model.pth')\n",
        "        return self.model, val_acc_history\n",
        "\n",
        "    #def forward(self, x):\n",
        "    #    return self.model(x)\n",
        "\n",
        "    def evaluate(self, testloader):\n",
        "        \"\"\"\n",
        "        train_methode is written in a way that it can also directly be used as evaluation\n",
        "        by calling it this way\n",
        "        \"\"\"\n",
        "        _, acc = self.train_model('_', testloader, '_', '_', num_epochs=1, is_inception=False)\n",
        "        return acc\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = torch.load(path)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t9ktEH8z8jV"
      },
      "source": [
        "model_obj = VGG(model_name='vgg11_bn', num_classes=num_classes, feature_extract=True, use_pretrained=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZCPCQFjz8ua",
        "outputId": "3fe27720-bb6c-46c0-b92d-ce146e6d4779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now we finetune the model to get a descent classfication performance on it\n",
        "print(\"Original model training:\")\n",
        "params_to_update = model_obj.get_params_to_update()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original model training:\n",
            "\tWeights and Bias to be tuned:\n",
            "\t\t classifier.6.weight\n",
            "\t\t classifier.6.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zGOBgjlz8w9"
      },
      "source": [
        "#optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "num_epochs = 10"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FtsPoEJ0mRR"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-4t6uC_0n8P",
        "outputId": "299b7d15-08b7-41a3-96b2-a43a4a2fbdc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model, hist = model_obj.train_model(train_loader, val_loader, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 0.4397 Acc: 0.8836\n",
            "val Loss: 0.4803 Acc: 0.9160\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA1fjErp0n-4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sBGPR6x5_I6"
      },
      "source": [
        "# Test performance on Test Set for Problem Domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvgosbxO0oBM"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/DHBW Vorlesungen/6. Semester/Daten für Projekte/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pErKgLv60oDe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzE_E5uNWDO7"
      },
      "source": [
        "# Train own VGG\n",
        "Training own VGG11 on Cifar 100, loading the pretrained model was diffult in a way that ART could properly interact with it\n",
        "\n",
        "Used for this pretraining was code from https://github.com/kuangliu/pytorch-cifar which was fitted for this usecasse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHPmFjte0nev"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eAhdpQFF1KV"
      },
      "source": [
        "'''\n",
        "VGG11/13/16/19 in Pytorch.\n",
        "https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
        "'''\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class Own_VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(Own_VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = Own_VGG('VGG11')\n",
        "    x = torch.randn(2,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLoSIE7pHT7s"
      },
      "source": [
        "Own_VGG.train_model = VGG.train_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Odo464AHk37"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = Own_VGG('VGG11')\n",
        "net = net.to(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e61Htl6MHdzY"
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "start_epoch = 1\n",
        "num_epochs = 5\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    if not (epoch % 10) or epoch == 1:\n",
        "        print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if not (epoch % 10) or epoch == 1:\n",
        "            print(batch_idx, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total), end='\\r')\n",
        "    if not (epoch % 10):\n",
        "        print()\n",
        "\n",
        "\n",
        "def eval(epoch, best_acc):\n",
        "\n",
        "    net.eval()\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if not (epoch % 10):\n",
        "                print(batch_idx, len(val_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total), end='\\r')\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        \n",
        "        torch.save(state, '/content/drive/MyDrive/DHBW Vorlesungen/6. Semester/Daten für Projekte/own_vgg_model.pth')\n",
        "        best_acc = acc\n",
        "    \n",
        "    return best_acc\n",
        "\n",
        "\n",
        "if False:\n",
        "    best_acc = 50\n",
        "    for epoch in range(start_epoch, start_epoch+100):\n",
        "        train(epoch)\n",
        "        best_acc = eval(epoch, best_acc)\n",
        "        scheduler.step()\n",
        "\n",
        "if True:\n",
        "    #net.load_state_dict(torch.load('/content/drive/MyDrive/DHBW Vorlesungen/6. Semester/Daten für Projekte/own_vgg_model.pth')['net'])\n",
        "    net.load_state_dict(torch.load('/content/drive/MyDrive/DHBW Vorlesungen/6. Semester/Daten für Projekte/own_vgg_model.pth', map_location=torch.device('cpu'))['net'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNIdj7CaN6RP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}