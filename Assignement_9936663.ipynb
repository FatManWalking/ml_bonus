{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Björns Bonus.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatManWalking/ml_bonus/blob/main/Bj%C3%B6rns_Bonus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI316RGyTNUi"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox --quiet\n",
        "\n",
        "#!pip install pytorch==1.8.1 --quiet"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIbDP4HcVf0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ec1b0e-d9fd-4810-f860-e8c57d8272f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdBwR0DdlvMC"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRfw8A8lTfIA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Sequential\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMtaDVeSqtiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282ce6f2-922b-40b4-9af8-d57f55f84c7f"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2cgGYdu3vPI"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzfSxX_z05Z7"
      },
      "source": [
        "# Get victim\n",
        "We only know that this classifier classifies bees and ants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHqKj1Ho04sc"
      },
      "source": [
        "victim = models.resnet18(pretrained=False)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd69tBAJ13Kd"
      },
      "source": [
        "victim = torch.load('/content/drive/MyDrive/Daten für Projekte/modelFinetune.pth')\n",
        "victim = victim.to(device)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pg6qmvm215V"
      },
      "source": [
        "# Get the batch loader and set up our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPImhjTU1Wb"
      },
      "source": [
        "def get_data_loader(batch_size=20):\n",
        "    \"\"\"\n",
        "    define data augmentation and return data loader with the cifar10 dataset loaded\n",
        "    parameters:\n",
        "        batch_size: size of the batch used for training and validation \n",
        "                    size of test batch is always 10\n",
        "    Return:\n",
        "        3 pytorch.dataloaders for easy batching of the dataset (train, validation and test)\n",
        "    \"\"\"\n",
        "\n",
        "    # data augmentation\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize([224,224]), # Resizing the image as the VGG only take 224 x 244 as input size\n",
        "            transforms.RandomHorizontalFlip(), # Flip the data horizontally\n",
        "            #TODO if it is needed, add the random crop\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    \"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "        \n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "    test = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=False, transform=transform_test)\n",
        "    \n",
        "    n = len(trainset)\n",
        "    print(n)\n",
        "    train, val =torch.utils.data.random_split(trainset, \n",
        "                                                    [int(n//100*60), int(n//100*40)],\n",
        "                                                    generator=torch.Generator().manual_seed(42))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=20, shuffle=True, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivfuzCoOU6JH",
        "outputId": "827e2f16-a285-43e1-bbf1-0749e1ee811c"
      },
      "source": [
        "train_loader, val_loader, test_loader = get_data_loader(batch_size=20)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8aEy8hiU8oC"
      },
      "source": [
        "model_name = 'resnet18'\n",
        "num_classes = 2"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QznaTxNvVKde"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name='vgg11', num_classes='10', feature_extract=True, use_pretrained=True):\n",
        "        \"\"\"\n",
        "        initalizes the version of VGG to be used and freezes the layers of the model and then add a new unfrozen head\n",
        "\n",
        "        params:\n",
        "            model_name(str): for example vgg16_bn for VGG-net with 16 weight layers and batch normalization\n",
        "            num_classes(int): number of output classes (100 when using CIFAR100)\n",
        "            feature_extract(bool): should the layers be frozen or not\n",
        "            use_pretrained(bool): use the pretrained model weights or only the architecture itself\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if model_name == 'vgg11':\n",
        "            model = models.vgg11(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg11_bn':\n",
        "            model = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg16':\n",
        "            model = models.vgg16(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg16_bn':\n",
        "            model = models.vgg16_bn(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg19':\n",
        "            model = models.vgg19(pretrained=use_pretrained)\n",
        "        elif model_name == 'vgg19_bn':\n",
        "            model = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        elif model_name == 'resnet18':\n",
        "            model = models.resnet18(pretrained=use_pretrained)\n",
        "            #print(model)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model\n",
        "        \n",
        "        \n",
        "        self.set_parameter_requires_grad(feature_extract)\n",
        "        input_size = 224\n",
        "\n",
        "        if model_name != 'resnet18':\n",
        "          num_ftrs = model.classifier[6].in_features\n",
        "          model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "          \n",
        "        \n",
        "        else:\n",
        "          model.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.feature_extract = feature_extract\n",
        "        self.input_size = input_size\n",
        "    \n",
        "    def get_params_to_update(self):\n",
        "        \"\"\" \n",
        "        prints the list of weights and bias that not frozen and will be tuned\n",
        "        \n",
        "        Gather the parameters to be optimized/updated in this run. If we are\n",
        "        finetuning we will be updating all parameters. However, if we are\n",
        "        doing feature extract method, we will only update the parameters\n",
        "        that we have just initialized, i.e. the parameters with requires_grad\n",
        "        is True.\n",
        "        \"\"\"\n",
        "\n",
        "        params_to_update = self.model.parameters()\n",
        "        print(\"\\tWeights and Bias to be tuned:\")\n",
        "        if self.feature_extract:\n",
        "            params_to_update = []\n",
        "            for name,param in self.model.named_parameters():\n",
        "                if param.requires_grad == True:\n",
        "                    params_to_update.append(param)\n",
        "                    print(\"\\t\\t\",name)\n",
        "        else:\n",
        "            for name,param in self.model.named_parameters():\n",
        "                if param.requires_grad == True:\n",
        "                    print(\"\\t\\t\",name)\n",
        "                    \n",
        "        return params_to_update\n",
        "    \n",
        "    def set_parameter_requires_grad(self, feature_extracting):\n",
        "        \"\"\"\n",
        "        this freezes the layers in the network\n",
        "        the new layer added later will automatically be unfrozen\n",
        "        \"\"\"\n",
        "        if feature_extracting:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "\n",
        "    # The trick is instead of using the real label we let the victim classify it first and use that label to train the classifier\n",
        "    def train_model(self, train_loader, val_loader, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "        since = time.time()\n",
        "\n",
        "        val_acc_history = []\n",
        "        val_loss_history = []\n",
        "\n",
        "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "        best_acc = 0.0\n",
        "        stop = False\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    self.model.train()  # Set model to training mode\n",
        "                    dataloader = train_loader\n",
        "                else:\n",
        "                    self.model.eval()   # Set model to evaluate mode\n",
        "                    dataloader = val_loader\n",
        "              \n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, _ in dataloader:\n",
        "                    inputs = inputs.to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        labels = np.argmax(victim(inputs).to('cpu'), axis=1)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Get model outputs and calculate loss\n",
        "                        \n",
        "                        outputs = self.model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                epoch_loss = running_loss / len(dataloader.dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "                if phase == 'val':\n",
        "                    val_acc_history.append(epoch_acc)\n",
        "                    # A simple addition to stop the training early\n",
        "                    val_loss_history.append(loss)\n",
        "                    if len(val_loss_history) > 3:\n",
        "                        stop = (abs(val_loss_history[-3] - val_loss_history[-3]) +\n",
        "                                abs(val_loss_history[-1] - val_loss_history[-2])) <= 0.001\n",
        "                        print(abs(val_loss_history[-3] - val_loss_history[-3]) +\n",
        "                                abs(val_loss_history[-1] - val_loss_history[-2]))\n",
        "                if stop:\n",
        "                    break\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "        # load best model weights\n",
        "        self.model.load_state_dict(best_model_wts)\n",
        "        torch.save(self.model, 'res_model.pth')\n",
        "        return self.model, val_acc_history\n",
        "\n",
        "    #def forward(self, x):\n",
        "    #    return self.model(x)\n",
        "\n",
        "    def evaluate(self, testloader):\n",
        "        \"\"\"\n",
        "        train_methode is written in a way that it can also directly be used as evaluation\n",
        "        by calling it this way\n",
        "        \"\"\"\n",
        "        _, acc = self.train_model('_', testloader, '_', '_', num_epochs=1, is_inception=False)\n",
        "        return acc\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = torch.load(path)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t9ktEH8z8jV"
      },
      "source": [
        "model_obj = VGG(model_name='resnet18', num_classes=num_classes, feature_extract=True, use_pretrained=True)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZCPCQFjz8ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffb75a3-85ae-4298-cd9b-767eab94fcaa"
      },
      "source": [
        "# Now we finetune the model to get a descent classfication performance on it\n",
        "print(\"Original model training:\")\n",
        "params_to_update = model_obj.get_params_to_update()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original model training:\n",
            "\tWeights and Bias to be tuned:\n",
            "\t\t fc.weight\n",
            "\t\t fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zGOBgjlz8w9"
      },
      "source": [
        "#optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "num_epochs = 20"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FtsPoEJ0mRR"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bevZdJgJa7KS"
      },
      "source": [
        "#model_obj.load_model('/content/drive/MyDrive/Daten für Projekte/stolenResNet.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-4t6uC_0n8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cad1b00-8273-459a-e8ae-c8762a9941ea"
      },
      "source": [
        "# We now train the model with the Cifar Dataset but instead of the real labels we use the labels the victim classifier would give them\n",
        "\n",
        "model, hist = model_obj.train_model(train_loader, val_loader, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 0.3129 Acc: 0.9134\n",
            "val Loss: 0.3139 Acc: 0.9095\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 0.3120 Acc: 0.9094\n",
            "val Loss: 0.3133 Acc: 0.9092\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 0.2997 Acc: 0.9141\n",
            "val Loss: 0.3086 Acc: 0.9087\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 0.3062 Acc: 0.9105\n",
            "val Loss: 0.3108 Acc: 0.9090\n",
            "tensor(0.0574, device='cuda:0')\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 0.3117 Acc: 0.9106\n",
            "val Loss: 0.3149 Acc: 0.9136\n",
            "tensor(0.0677, device='cuda:0')\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 0.3067 Acc: 0.9128\n",
            "val Loss: 0.3296 Acc: 0.9156\n",
            "tensor(0.3374, device='cuda:0')\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 0.3039 Acc: 0.9134\n",
            "val Loss: 0.3593 Acc: 0.9156\n",
            "tensor(0.4996, device='cuda:0')\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 0.3081 Acc: 0.9107\n",
            "val Loss: 0.3511 Acc: 0.8868\n",
            "tensor(0.3128, device='cuda:0')\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 0.3035 Acc: 0.9135\n",
            "val Loss: 0.3296 Acc: 0.9150\n",
            "tensor(0.1839, device='cuda:0')\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 0.3019 Acc: 0.9126\n",
            "val Loss: 0.3001 Acc: 0.9133\n",
            "tensor(0.1743, device='cuda:0')\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 0.3054 Acc: 0.9116\n",
            "val Loss: 0.3095 Acc: 0.9131\n",
            "tensor(0.0795, device='cuda:0')\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 0.3062 Acc: 0.9107\n",
            "val Loss: 0.3024 Acc: 0.9120\n",
            "tensor(0.3693, device='cuda:0')\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 0.3032 Acc: 0.9133\n",
            "val Loss: 0.3141 Acc: 0.9014\n",
            "tensor(0.1406, device='cuda:0')\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 0.3022 Acc: 0.9133\n",
            "val Loss: 0.3018 Acc: 0.9118\n",
            "tensor(0.2575, device='cuda:0')\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 0.3012 Acc: 0.9136\n",
            "val Loss: 0.3132 Acc: 0.9067\n",
            "tensor(0.5797, device='cuda:0')\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 0.3061 Acc: 0.9115\n",
            "val Loss: 0.3101 Acc: 0.9061\n",
            "tensor(0.2490, device='cuda:0')\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 0.3048 Acc: 0.9127\n",
            "val Loss: 0.2991 Acc: 0.9123\n",
            "tensor(0.0168, device='cuda:0')\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 0.3063 Acc: 0.9111\n",
            "val Loss: 0.3253 Acc: 0.9117\n",
            "tensor(0.1931, device='cuda:0')\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 0.3019 Acc: 0.9125\n",
            "val Loss: 0.3360 Acc: 0.9156\n",
            "tensor(0.1822, device='cuda:0')\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 0.3040 Acc: 0.9132\n",
            "val Loss: 0.3128 Acc: 0.9057\n",
            "tensor(0.2730, device='cuda:0')\n",
            "\n",
            "Training complete in 16m 7s\n",
            "Best val Acc: 0.915600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA1fjErp0n-4"
      },
      "source": [
        "torch.save(model, '/content/drive/MyDrive/Daten für Projekte/stolenResNet.pth')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sBGPR6x5_I6"
      },
      "source": [
        "# Test performance on Test Set for Problem Domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTvU7QVmYFcx"
      },
      "source": [
        "import os\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvgosbxO0oBM",
        "outputId": "6ba79f8b-34d3-4272-e72a-591bc81f1153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/Daten für Projekte/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(class_names)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ants', 'bees']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pErKgLv60oDe"
      },
      "source": [
        "def eval(model, dataloaders, phase):\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloaders[phase]):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            #print(predicted, targets)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if not batch_idx % 10:\n",
        "                print(len(dataloaders[phase]), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                            % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "                \n",
        "    return 100.*correct/total"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2mJN3jkZyYw",
        "outputId": "05795ba0-6777-47c5-9332-c6900d582ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "eval(model, dataloaders, 'val')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-48186074d156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7P9PaXyavwx",
        "outputId": "79a2be7f-9bfe-4501-ceaa-05317877ed78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "eval(victim, dataloaders, 'val')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39 Loss: 0.552 | Acc: 100.000% (4/4)\n",
            "39 Loss: 0.576 | Acc: 79.545% (35/44)\n",
            "39 Loss: 0.557 | Acc: 78.571% (66/84)\n",
            "39 Loss: 0.581 | Acc: 75.000% (93/124)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72.54901960784314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYhYhgfPaUqt"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho0PWCeQapYv"
      },
      "source": [
        "victim"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}